# -*- coding: utf-8 -*-
"""nlp_project_dicoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vt_iDFxYJeDYWMJOJrxazhcfJaZPnoea

# Tweets Sentiment Classifcation Dataset
Denny Alvito Ginting
dennyginting73@gmail.com


#### Acknowledge
This dataset was taken from [Kaggle](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)

## Importing Libraries
"""

# Dataframe and Linear Algebra
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Splitting Dataset
from sklearn.model_selection import train_test_split

# Natural Language Processing
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
all_stopwords = stopwords.words('english')
from nltk.stem.wordnet import WordNetLemmatizer
import gensim

# Modelling
import tensorflow as tf
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential, Input
from keras.layers import Embedding, GlobalAveragePooling1D, Dense, LSTM, Dropout, Bidirectional, BatchNormalization
from keras.regularizers import L2

# TensorBoard
import datetime
import os

"""## Impoting Dataset"""

df = pd.read_csv('https://raw.githubusercontent.com/DnYAlv/Intermediate_Dicoding/main/dataset/twitter_training.csv',
                 names=['tweet_id','entity','sentiment','tweets'])
df.head()

"""## Feature Selection
We only need the sentiment and tweets
"""

df.drop(['tweet_id','entity'], axis=1, inplace=True)

df.isnull().sum()

"""## Dropping Missing Values"""

df.dropna(inplace=True)

df.reset_index(drop=True, inplace=True)

"""## Exploring Data

### Checking the distribution of news
"""

plt.figure(figsize=(12,6))

ax = sns.countplot(x='sentiment', data=df)
plt.xlabel('Sentiment')
plt.title('Sentiment Distribution')
plt.show()

"""## Dropping Irrelevant Sentiment
- Dropping this sentiment since it won't help our model to predict
"""

df = df[df['sentiment'] != 'Irrelevant']

df.reset_index(drop=True, inplace=True)

"""## Preprocessing"""

def clean_text(data):
  list_text = []
  for i in range(len(data)):
    
    # Remove URLs
    text1 = re.sub(r'https\S+',' ', data['tweets'][i])

    # Remove HTML tag
    text2 = re.sub(r'<.*?>',' ',text1)

    # Remove Numbers
    text3 = re.sub(r'\d+',' ', text2)
    
    # Remove Punctuations
    text4 = re.sub('[^a-zA-Z]',' ', text3)

    # Lowering Alphabet
    token_text = text4.lower()
    
    # Tokenizing
    token_text = token_text.split()

    # Lemmatizing and Removing StopWords
    lemmatizer = WordNetLemmatizer()
    lemmatized_text = [lemmatizer.lemmatize(word) for word
                       in token_text if word not in all_stopwords]
    
    # Joining back (List of tokens to sentence)
    cleaned_text = ' '.join(lemmatized_text)

    # Appending the cleaned text
    list_text.append(cleaned_text)
  
  return list_text

clean_data = clean_text(df)

df.insert(1, 'clean_text', clean_data)
df.head()

"""### Dropping the uncleaned text"""

data = df.drop('tweets', axis=1)
data.head()

"""### Finding how many Unique Tokens"""

list_of_words = []

for sentence in data['clean_text']:
  tokens = sentence.split()
  for token in tokens:
    list_of_words.append(token)

unique_words = len(list(set(list_of_words)))
print(f'There are {unique_words} unique words (Vocabulary)')

"""### Finding the maximum and average number of words"""

MAX_LENGTH = -1
list_of_token_length = []
for sentence in data['clean_text']:
  tokens = sentence.split()
  list_of_token_length.append(len(tokens))

  if MAX_LENGTH < len(tokens):
    MAX_LENGTH = len(tokens)
  
print(f'Maximum Number Of Words: {MAX_LENGTH}')
print(f'Average Number Of Words: {np.mean(list_of_token_length)}')

ctr = 0
avg = np.mean(list_of_token_length)
for num in list_of_token_length:
  if num > avg:
    ctr+=1

print(f'There are {ctr} sentences ({round(ctr/(data.shape[0])*100,2)}% of Full Dataset) that have more than average number of words')

"""> There are almost half (40.05 %) of dataset that have length more than average. Therefore we will set that the `sentence` must equal to the `Maximum Number of Words` (Padded Process)"""

plt.figure(figsize=(10,6))
sns.histplot(data=pd.Series(list_of_token_length))
plt.title('Distribution Number of Words')
plt.xlabel('Number of Words')
plt.show()

"""> However, we can see that maximum number is rarely exist, instead, we can use 40 as maximum number"""

MAX_LENGTH = 40

"""### One Hot on Sentiment"""

sentiment = pd.get_dummies(data['sentiment'])
new_data = pd.concat([data['clean_text'], sentiment], axis=1)
new_data.head()

X = new_data['clean_text'].values
y = new_data[['Negative','Neutral','Positive']].values

"""## Splitting Dataset"""

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42)

"""## Tokenizing"""

tokenizer = Tokenizer(num_words=unique_words, oov_token='<oov>')
tokenizer.fit_on_texts(X_train)

# Sequence text
train_sequence = tokenizer.texts_to_sequences(X_train)
test_sequence = tokenizer.texts_to_sequences(X_test)

# Padding sequence
train_padded = pad_sequences(train_sequence,
                             padding='post',
                             maxlen=MAX_LENGTH+1,
                             truncating='post')
test_padded = pad_sequences(test_sequence,
                            padding='post',
                            maxlen=MAX_LENGTH+1,
                            truncating='post')

"""## Modelling (LSTM)"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

list_of_sentiment = ['Negative','Neutral','Positive']
classes = len(list_of_sentiment)

tf.keras.backend.clear_session()
model = Sequential([
    Embedding(input_dim=unique_words, output_dim=50, input_length=MAX_LENGTH+1),
    LSTM(50, dropout=0.2),
    Dense(100, activation='relu', kernel_regularizer=L2(0.001)),
    Dense(50, activation='relu', kernel_regularizer=L2(0.001)),
    Dropout(0.2),
    Dense(25, activation='relu', kernel_regularizer=L2(0.001)),
    Dense(classes, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

model.summary()

ES = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                      mode='min',
                                      verbose=1,
                                      patience=50,
                                      restore_best_weights=True)
checkpoint = tf.keras.callbacks.ModelCheckpoint('weights.hdf5', monitor='val_accuracy',
                                                save_best_only=True)

logdir = os.path.join('logs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

history = model.fit(
    train_padded, y_train,
    epochs = 100,
    batch_size=64,
    validation_data=(test_padded, y_test),
    callbacks=[checkpoint, ES, tensorboard_callback],
    verbose=1
)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

model.save('bestModel.h5')

bestModel = tf.keras.models.load_model('bestModel.h5')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,6))

ax[0].plot(acc, '-', label = 'Training Accuracy')
ax[0].plot(val_acc, '-', label = 'Validation Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].set_title('Epochs & Training Accuracy', fontsize=20)
ax[0].legend(loc='best')

ax[1].plot(loss, '-', label = 'Training loss')
ax[1].plot(val_loss, '-', label = 'Validation loss')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('loss')
ax[1].set_title('Epochs & Training loss', fontsize=20)
ax[1].legend(loc='best')

plt.show()

model.load_weights('weights.hdf5')
evaluate = model.evaluate(test_padded, y_test)
print(f'Test Set\n Loss: {evaluate[0]} Accuracy: {evaluate[1]}')